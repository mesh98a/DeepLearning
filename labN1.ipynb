{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8720c650baefd2e",
   "metadata": {},
   "source": [
    "# Medical Cost Personal Datasets ( Variant 4)\n",
    "\n",
    "Name: Meshalkin Artur <br>\n",
    "Student-ID : 586811\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T17:24:09.355814Z",
     "start_time": "2025-07-18T17:24:09.351153Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54ce8d69e18530a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T17:24:09.390601Z",
     "start_time": "2025-07-18T17:24:09.366312Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/insurance.csv') # read csv data as dataframe\n",
    "df.head() # first 5 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9062f34a2d59169",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T17:24:09.459431Z",
     "start_time": "2025-07-18T17:24:09.448860Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info() # information about columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b9571cfe0e0e1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T17:24:09.578002Z",
     "start_time": "2025-07-18T17:24:09.556748Z"
    }
   },
   "outputs": [],
   "source": [
    "df.describe() # numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5694fb1d5dc89f31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T17:24:09.695874Z",
     "start_time": "2025-07-18T17:24:09.683185Z"
    }
   },
   "outputs": [],
   "source": [
    "df.describe(include='object') # categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b62de52ab672db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T17:24:09.793869Z",
     "start_time": "2025-07-18T17:24:09.778607Z"
    }
   },
   "outputs": [],
   "source": [
    "df[df.duplicated(keep=False)] # check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2518ce871632063",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T17:24:10.099005Z",
     "start_time": "2025-07-18T17:24:10.091594Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True) # remove duplicates\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3a7cda87f7d2e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T17:24:10.183955Z",
     "start_time": "2025-07-18T17:24:10.174164Z"
    }
   },
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True, drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae27588234965b58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T17:24:10.321281Z",
     "start_time": "2025-07-18T17:24:10.311697Z"
    }
   },
   "outputs": [],
   "source": [
    "df.isnull().mean() * 100 # check for null values in percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3da2dfb7b59fa2",
   "metadata": {},
   "source": [
    "# Numerical data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d9ec521740fe2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T17:24:13.879952Z",
     "start_time": "2025-07-18T17:24:10.413136Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdea858a5fcd68f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T17:24:14.258077Z",
     "start_time": "2025-07-18T17:24:14.003764Z"
    }
   },
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "sns.heatmap(corr, annot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a26f8974f31c74a",
   "metadata": {},
   "source": [
    "# Categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556a0927d57b9ace",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T17:24:15.010700Z",
     "start_time": "2025-07-18T17:24:14.300200Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_cat(x):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    colors = sns.color_palette(\"winter\", 2)\n",
    "    sns.boxplot(ax=axes[0], data=df, x=x, y=\"charges\", palette=\"winter\")\n",
    "    axes[0].set_xlabel(x, fontsize=12, color='blue')\n",
    "    axes[0].set_ylabel(\"Charges\", fontsize=12, color='blue')\n",
    "    axes[0].grid(True, linestyle='--', color='lavender')\n",
    "    smoker_counts = df[x].value_counts()\n",
    "    axes[1].pie(\n",
    "        smoker_counts,\n",
    "        labels=smoker_counts.index,\n",
    "        autopct='%1.1f%%',\n",
    "        colors=colors,\n",
    "        startangle=120,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "display_cat(\"smoker\")\n",
    "display_cat(\"sex\")\n",
    "display_cat(\"region\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4095a8290aa20196",
   "metadata": {},
   "source": [
    "# Target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0375a941d61343",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T17:24:15.315538Z",
     "start_time": "2025-07-18T17:24:15.017359Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "sns.histplot(df,x=\"charges\",bins=20,kde=True);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4f4e8d19fdb2f4",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0060dd448a6f2d3",
   "metadata": {},
   "source": [
    "## StandardScaler\n",
    "\n",
    "The **StandardScaler** standardizes features by removing the mean and scaling to unit variance.\n",
    "\n",
    "The formula used is:\n",
    "$$\n",
    "z = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "Where:\n",
    "- $x$ : the original value\n",
    "- $\\mu$: the mean of the feature\n",
    "- $\\sigma$: the standard deviation of the feature\n",
    "- $z$: the standardized value\n",
    "\n",
    "This transformation results in a feature with a mean of 0 and a standard deviation of 1s\n",
    "\n",
    "\n",
    "The **OneHotEncoder** is a method for converting categorical variables into a binary format.\n",
    "It creates new columns for each category where 1 means the category is present and 0 means it is not <br>\n",
    "The **OneHotEncoder(drop=\"first\")** transforms categorical variables into binary (0/1) columns, dropping the first category to avoid redundancy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cae99070cad5efe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T17:24:15.619991Z",
     "start_time": "2025-07-18T17:24:15.337975Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "categorical_cols = [\"sex\", \"smoker\", \"region\"]\n",
    "numeric_cols = [\"age\", \"bmi\", \"children\"]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_cols),\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\"), categorical_cols)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf9b545a81cd807",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78b6f744df5a1dc",
   "metadata": {},
   "source": [
    "Linear Regression is a supervised learning algorithm used to predict a continuous target variable based on the linear relationship between input features (independent variables) and the output (dependent variable).\n",
    "\n",
    "\n",
    "**Multiple Linear Regression** :\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\hat{y}$ : predicted value\n",
    "- $\\beta_0$ : bias\n",
    "- $\\beta_1, \\beta_2, \\beta_n $ : weights\n",
    "- $x_1 , x_2, x_n$: features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0bcbcd26f5f5c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T17:24:16.224258Z",
     "start_time": "2025-07-18T17:24:15.626334Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "lin_model = Pipeline(steps=[\n",
    "    (\"preprocessing\", preprocessor),\n",
    "    (\"regressor\", LinearRegression())\n",
    "])\n",
    "\n",
    "X = df.drop(['charges'], axis=1)\n",
    "y = df[\"charges\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "lin_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = lin_model.predict(X_train)\n",
    "y_test_pred = lin_model.predict(X_test)\n",
    "\n",
    "\n",
    "def plot_charges_lin():\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_test_pred, y_test, alpha=0.6, color='steelblue', edgecolor='black', label='Actual vs Predicted')\n",
    "    plt.plot(y_test_pred, y_test_pred, color='red', linewidth=2, label='Perfect Prediction')\n",
    "    plt.xlabel(\"Predicted Charges\", fontsize=12)\n",
    "    plt.ylabel(\"Actual Charges\", fontsize=12)\n",
    "    plt.title(\"Linear Regression: Predicted vs Actual Charges\", fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_charges_lin()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1012bc33eb92432",
   "metadata": {},
   "source": [
    "## Loss Function (for training)\n",
    "Training linear regression is the minimization of the mean squared error(MSE)\n",
    "\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "-  $y_i$: actual value\n",
    "- $\\hat{y}_i$: predicted value\n",
    "- $n$: number of data points\n",
    "\n",
    "After training, we often use the same MSE to evaluate how well the model is performing on test data. Alternatively, we can use other metrics.\n",
    "\n",
    "## Metrics (for evaluation)\n",
    "### **Root Mean Squared Error (RMSE)**\n",
    "$$\n",
    "RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n",
    "$$\n",
    "- Same units as the target variable\n",
    "### **Mean Absolute Error (MAE) , Mean Absolute Percentage Error (MAPE)**\n",
    "$$\n",
    "MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "$$\n",
    "- Less sensitive to outliers than MSE\n",
    "$$\n",
    "MAPE = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{|y_i - \\hat{y}_i|}{y_i}\n",
    "$$\n",
    "### **R-squared (Coefficient of Determination)**\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} {\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n",
    "$$\n",
    "- $\\bar{y}$ : the mean of the predicted value\n",
    "- Indicates proportion of variance explained by the model\n",
    "- Value ranges from 0 to 1 (higher is better)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1d070a114461ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T17:24:16.264859Z",
     "start_time": "2025-07-18T17:24:16.254294Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "\n",
    "def display_metrics(y_test,y_test_pred,y_train,y_train_pred):\n",
    "    mse_train = mean_squared_error(y_train,y_train_pred)\n",
    "    mse = mean_squared_error(y_test,y_test_pred)\n",
    "\n",
    "    rmse_train = np.sqrt(mse_train)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "    mape_train = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_test_pred)\n",
    "\n",
    "    print(f\"Train MSE: {round(mse_train,3)} RMSE: {round(rmse_train,3)}, R²: {round(r2_train,3)}, MAPE: {round(mape_train,3)}\")\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"Test MSE:{round(mse,3)} RMSE: {round(rmse,3)}, R²: {round(r2,3)}, MAPE: {round(mape,3)}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "\n",
    "display_metrics(y_test,y_test_pred,y_train,y_train_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fac3adee2ff752f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T17:24:16.357720Z",
     "start_time": "2025-07-18T17:24:16.293280Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(lin_model, X, y, cv=5, scoring='r2')\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532dd1548a6b71c1",
   "metadata": {},
   "source": [
    "## Learning Curves\n",
    "Learning curves help us understand how well our model is learning as you increase the amount of training data.\n",
    "\n",
    "They typically show two lines:\n",
    "\n",
    "    Training score – the model's performance  on the training set.\n",
    "\n",
    "    Test score – the model's performance on the test set.\n",
    "The shaded area around the learning curves, typically representing the standard deviation (error band).\n",
    "The smaller the standard deviation, the more stable is the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b348b2506c0c1fb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T17:24:18.322181Z",
     "start_time": "2025-07-18T17:24:16.391115Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "\n",
    "def plot_learning_curve(X, y, model,scoring):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator=model,\n",
    "        X=X,\n",
    "        y=y,\n",
    "        scoring=scoring,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 5)\n",
    "    )\n",
    "    train_mean = train_scores.mean(axis=1)\n",
    "    test_mean = test_scores.mean(axis=1)\n",
    "    train_std = train_scores.std(axis=1)\n",
    "    test_std = test_scores.std(axis=1)\n",
    "    plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color=\"g\") # error band train\n",
    "    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color=\"r\") # error band test\n",
    "\n",
    "    plt.plot(train_sizes, train_mean, label=\"train\", marker='o', color='g')\n",
    "    plt.plot(train_sizes, test_mean, label=\"test\", marker='o', color='r')\n",
    "\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.xlabel(\"Training Set Size\")\n",
    "    plt.ylabel(scoring)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_learning_curve(X,y,lin_model,scoring=\"r2\")\n",
    "plot_learning_curve(X,y,lin_model,scoring=\"neg_root_mean_squared_error\")\n",
    "plot_learning_curve(X,y,lin_model,scoring=\"neg_mean_absolute_percentage_error\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dc3fe076bdfe55",
   "metadata": {},
   "source": [
    "# Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b549e4abc31d2f5b",
   "metadata": {},
   "source": [
    "- Polynomial regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modeled as a polynomial in x\n",
    "$$\n",
    "\\hat{y} = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\dots + \\beta_n x^n\n",
    "$$\n",
    "\n",
    "**GridSearchCV** is  used to tune hyperparameters .\n",
    "\n",
    "It performs **cross-validation** for each combination of hyperparameters and returns the combination that gives the best performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747b78521c88d6c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T17:24:19.425065Z",
     "start_time": "2025-07-18T17:24:18.346201Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "poly_model = Pipeline(steps=[\n",
    "    (\"preprocessing\", preprocessor),\n",
    "    (\"poly_features\", PolynomialFeatures(include_bias=False)),\n",
    "    (\"regressor\", LinearRegression())\n",
    "])\n",
    "param_grid = {\n",
    "    \"poly_features__degree\": [1, 2, 3, 4, 5]\n",
    "}\n",
    "grid_search = GridSearchCV(poly_model, param_grid=param_grid, cv=5, scoring=\"r2\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "results = grid_search.cv_results_\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "results_df[[\"params\", \"mean_test_score\", \"std_test_score\", \"rank_test_score\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933ea615b530b883",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T17:24:19.879852Z",
     "start_time": "2025-07-18T17:24:19.532463Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "def poly_model_predict(degree,preprocessor):\n",
    "    poly_model = Pipeline(steps=[\n",
    "        (\"preprocessing\", preprocessor),\n",
    "        (\"poly_features\", PolynomialFeatures(degree=degree, include_bias=False)),\n",
    "        (\"regressor\", LinearRegression())\n",
    "    ])\n",
    "    poly_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_poly_train = poly_model.predict(X_train)\n",
    "    y_pred_poly_test = poly_model.predict(X_test)\n",
    "\n",
    "    return y_pred_poly_train, y_pred_poly_test\n",
    "\n",
    "y_pred_poly_train, y_pred_poly_test = poly_model_predict(3,preprocessor)\n",
    "\n",
    "\n",
    "def plot_charges_poly():\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_pred_poly_test, y_test, alpha=0.6, color='steelblue', edgecolor='black', label='Actual vs Predicted')\n",
    "    plt.plot(y_pred_poly_test, y_pred_poly_test, color='red', linewidth=2, label='Perfect Prediction')\n",
    "    plt.xlabel(\"Predicted Charges\", fontsize=12)\n",
    "    plt.ylabel(\"Actual Charges\", fontsize=12)\n",
    "    plt.title(\"Polynomial Regression: Predicted vs Actual Charges\", fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_charges_poly()\n",
    "\n",
    "display_metrics(y_test,y_pred_poly_test,y_train,y_pred_poly_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad70d582221ded40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T17:24:22.148800Z",
     "start_time": "2025-07-18T17:24:19.924414Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(X,y,poly_model,scoring=\"r2\")\n",
    "plot_learning_curve(X,y,poly_model,scoring=\"neg_root_mean_squared_error\")\n",
    "plot_learning_curve(X,y,poly_model,scoring=\"neg_mean_absolute_percentage_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1faec37a6abf8ca",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "When using linear regression, it is evident that the model is underfitting (the results on the test data are even better than on the training data). When comparing linear regression to polynomial regression of degree 3, we can see that the polynomial model provides results that are closer to the ideal line (in the best case, the points lie along a 45-degree angle). All performance metrics have also significantly improved and the error band in the learning curve  gets smaller.\n",
    "- R² is 0.86\n",
    "This means that 86% of the variance in insurance charges is explained by the polynomial model.\n",
    "- An RMSE of approximately 4,928 compared to a standard deviation of around 12,100 is a very good result.\n",
    "- MAPE is highly sensitive to small target values, which can lead to inflated error percentages — that's likely why our MAPE is relatively high, and this isn't ideal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1ed2ea627bd330",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T17:24:24.043932Z",
     "start_time": "2025-07-18T17:24:22.158589Z"
    }
   },
   "outputs": [],
   "source": [
    "display_metrics(y_test,y_test_pred,y_train,y_train_pred)\n",
    "plot_charges_lin()\n",
    "plot_learning_curve(X,y,lin_model,scoring=\"r2\")\n",
    "\n",
    "display_metrics(y_test,y_pred_poly_test,y_train,y_pred_poly_train)\n",
    "plot_charges_poly()\n",
    "plot_learning_curve(X,y,poly_model,scoring=\"r2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb74fab704be5f8",
   "metadata": {},
   "source": [
    "*\n",
    "If we use a polynomial of degree 4 or higher, the model starts to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6663f08f97ef0c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T17:24:31.848729Z",
     "start_time": "2025-07-18T17:24:24.091591Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_poly_degree_performance(y_test,preprocessor, max_degree=10):\n",
    "    r2_scores = []\n",
    "\n",
    "    degrees = range(1, max_degree + 1)\n",
    "\n",
    "    for d in degrees:\n",
    "        y_train_pred , y_test_pred = poly_model_predict(d,preprocessor=preprocessor)\n",
    "\n",
    "        r2_scores.append(r2_score(y_test, y_test_pred))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(degrees, r2_scores, label='Test R² Score', marker='^', color='green')\n",
    "    plt.xlabel('Polynomial Degree')\n",
    "    plt.ylabel('R² Score')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title('R² vs Polynomial Degree')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_poly_degree_performance(y_test,preprocessor=preprocessor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
