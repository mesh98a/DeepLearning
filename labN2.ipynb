{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mesh98a/DeepLearning/blob/main/labN2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O8kvp95olWtk",
    "outputId": "566b250b-400a-4b51-a546-6b5c79b96016"
   },
   "outputs": [],
   "source": [
    "!wget -O data.zip https://archive.ics.uci.edu/static/public/461/drug+review+dataset+druglib+com.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vn88FKpolyBE"
   },
   "outputs": [],
   "source": [
    "!mkdir -p lab2\n",
    "!unzip -qq data.zip -d lab2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ceSRo5psQ2MN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, SimpleRNN, Embedding, LSTM, GRU, Bidirectional\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report, accuracy_score, balanced_accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gm4NwqGLQ7Mg"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('lab2/drugLibTrain_raw.tsv', delimiter='\\t')\n",
    "df_test = pd.read_csv('lab2/drugLibTest_raw.tsv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "p3SwOrnbnjIe",
    "outputId": "fe06460b-c989-43a6-e310-ebba7d311df6"
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzoeCBTtnx50"
   },
   "source": [
    "To make predictions based on reviews, all three columns are combined into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VHCpxWPCQ_wM",
    "outputId": "af9995cd-f6fd-479f-95f7-21b63fbf848b"
   },
   "outputs": [],
   "source": [
    "def combine_reviews(df):\n",
    "    return (\n",
    "        df['benefitsReview'].fillna('') + ' ' +\n",
    "        df['sideEffectsReview'].fillna('') + ' ' +\n",
    "        df['commentsReview'].fillna('')\n",
    "    )\n",
    "X_train_text = combine_reviews(df_train)\n",
    "X_test_text = combine_reviews(df_test)\n",
    "print(X_train_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAbpuineoQg6"
   },
   "source": [
    "In the first case, we will determine the medication rating using binary classification. For this, all ratings less than or equal to 5 are converted to 0, and those above 5 are converted to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRClu0RaREim"
   },
   "outputs": [],
   "source": [
    "def split_rating(df):\n",
    "    df['rating_category'] = np.where(df['rating'] <= 5, 0, 1)\n",
    "    return df\n",
    "\n",
    "df_train = split_rating(df_train)\n",
    "df_test = split_rating(df_test)\n",
    "\n",
    "y_train = df_train['rating_category']\n",
    "y_test = df_test['rating_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "QfRI0B9GUtfA",
    "outputId": "47039370-cb68-4ddf-990d-c1bf56e87e94"
   },
   "outputs": [],
   "source": [
    "df_train['rating_category'].value_counts() # class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JVEKC5xYpD93"
   },
   "source": [
    "Since models can only work with numbers, we need to convert text into numerical form. Using a tokenizer, we split the text into tokens (usually words) and assign them indices based on frequency, with 1 being the most frequent word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PPUzX8Q5RbMJ",
    "outputId": "cd0437af-82c2-4172-f01a-d044625ab8f7"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train_text)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "\n",
    "#print(tokenizer.index_word)\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcEitZvzrbKH"
   },
   "source": [
    "The next step is converting the text into numerical sequences and bringing them to the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJXRs2GIRgkj"
   },
   "outputs": [],
   "source": [
    "X_train_seq = tokenizer.texts_to_sequences(X_train_text)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test_text)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=200)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Su3Rx35svocJ"
   },
   "source": [
    "Next come classification metrics to evaluate the performance of our model during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skJKnrceijXS"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, history, X_test, y_test, is_binary=True):\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_probs = model.predict(X_test)\n",
    "    if is_binary:\n",
    "        y_pred = (y_pred_probs > 0.5).astype(int) # [0,1]\n",
    "    else:\n",
    "        y_pred = np.argmax(y_pred_probs, axis=1) # [0,1,2]\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    roc = roc_auc_score(y_test, y_pred_probs) if is_binary else roc_auc_score(y_test, y_pred_probs, multi_class='ovr')\n",
    "    prec = precision_score(y_test, y_pred, average='weighted')\n",
    "    rec = recall_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    print(f\"Accuracy: {acc:.3f}\")\n",
    "    print(f\"Balanced Accuracy: {bal_acc:.3f}\")\n",
    "    print(f\"F1 Score: {f1:.3f}\")\n",
    "    print(f\"ROC AUC: {roc:.3f}\")\n",
    "    print(f\"Precision: {prec:.3f}\")\n",
    "    print(f\"Recall: {rec:.3f}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Loss Curve\n",
    "    if 'loss' in history.history:\n",
    "        plt.plot(history.history['loss'], color='blue', label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], color='red', label='Validation Loss')\n",
    "        plt.title('Loss Curve')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xu-KK7myv208"
   },
   "source": [
    "In our binary classification task based on text reviews, we use several key components.\n",
    "\n",
    "First, we apply an Embedding layer to convert words into dense vector representations. This is necessary so the model can work with numerical features rather than raw text, capturing semantic relationships between words.\n",
    "\n",
    "Next, we use a Bidirectional RNN, which allows the model to analyze both the previous and the next context of each word in the sentence. This is especially important for understanding the meaning of phrases within the text.\n",
    "\n",
    "At the output, we use a layer with a sigmoid activation function, which returns the probability that a review belongs to the positive class.\n",
    "\n",
    "Since the dataset may be imbalanced (e.g., more positive reviews than negative ones), we also use compute_class_weight to automatically adjust class weights, helping the model to fairly learn both classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ZAn5funRm3d"
   },
   "outputs": [],
   "source": [
    "def train_binary_model(model_name, RNNLayer):\n",
    "    print(f\"Training: {model_name}\")\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size + 1, 100),\n",
    "        Bidirectional(RNNLayer(64, return_sequences=True)),\n",
    "        Dropout(0.3),\n",
    "        Bidirectional(RNNLayer(64)), # second layer\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "    history = model.fit(X_train_pad, y_train, epochs=10, batch_size=128, validation_data =(X_test_pad,y_test), class_weight=class_weights, verbose=1)\n",
    "\n",
    "    evaluate_model(model, history, X_test_pad, y_test, is_binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xslXrk-MT70Q",
    "outputId": "fde515b4-e55b-4c95-f87d-c2c8dd285bb5"
   },
   "outputs": [],
   "source": [
    "train_binary_model(\"SimpleRNN\", SimpleRNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qdgiZetIT-V3",
    "outputId": "9ea1b6f0-421b-4f73-a06f-b99b458beb1f"
   },
   "outputs": [],
   "source": [
    "train_binary_model(\"LSTM\", LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Hs4ytFx0T-27",
    "outputId": "3423bd90-5a61-445a-f865-a562889199f0"
   },
   "outputs": [],
   "source": [
    "train_binary_model(\"GRU\", GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMxdsr5FgWIF"
   },
   "source": [
    "Multiclassification (Effectiveness)\n",
    "We reduce the number of classes to three levels of effectiveness: Low, Moderate, and High."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "CrC_yviogWq1",
    "outputId": "764016c4-357d-4219-ee77-f5ee8c4e1c21"
   },
   "outputs": [],
   "source": [
    "def map_effectiveness(x):\n",
    "    if x in ['Highly Effective', 'Considerably Effective']:\n",
    "        return 'High'\n",
    "    elif x in ['Moderately Effective', 'Marginally Effective']:\n",
    "        return 'Moderate'\n",
    "    else:\n",
    "        return 'Low'\n",
    "\n",
    "eff_map = {'Low': 0, 'Moderate': 1, 'High': 2}\n",
    "\n",
    "df_train['eff_label'] = df_train['effectiveness'].apply(map_effectiveness).map(eff_map)\n",
    "df_test['eff_label'] = df_test['effectiveness'].apply(map_effectiveness).map(eff_map)\n",
    "df_train['eff_label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXDL8LzXFFd3"
   },
   "source": [
    "The effectiveness labels (eff_label), encoded as integers 0, 1, and 2, are converted into one-hot encoded vectors using to_categorical.\n",
    "This means each label becomes a vector of length 3, with a 1 at the position of the class index and 0s elsewhere.\n",
    "This format is required for training models on multiclass classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NjkG3bTG_CIb",
    "outputId": "bb09c690-d0f0-4292-e62f-63b0532326d9"
   },
   "outputs": [],
   "source": [
    "y_train_eff = df_train['eff_label']\n",
    "y_test_eff = df_test['eff_label']\n",
    "y_train_eff_cat = to_categorical(df_train['eff_label'], num_classes=3)\n",
    "y_test_eff_cat = to_categorical(df_test['eff_label'], num_classes=3)\n",
    "\n",
    "print(y_train_eff[0])\n",
    "print(y_train_eff_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2oAHWMz2idjm"
   },
   "outputs": [],
   "source": [
    "y_test_labels = df_test['eff_label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "akGWwnLQC3Wv"
   },
   "source": [
    "In this multiclass classification task we use a sequential neural model.\n",
    "\n",
    "We also use an Embedding layer and a Bidirectional RNN. In this case, dropout parameters are applied within the Bidirectional RNN to reduce overfitting. Additionally, a separate Dropout layer is added, which randomly deactivates some neurons during training to further prevent overfitting.\n",
    "\n",
    "The output layer is a Dense layer with softmax activation, which returns probabilities for each of the three classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EhcSSTymjw57"
   },
   "outputs": [],
   "source": [
    "def train_multiclass_model(name, RNNLayer):\n",
    "    print(f\"Training Multiclass: {name}\")\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size + 1, 100),\n",
    "        #Bidirectional(RNNLayer(64, return_sequences=True, dropout=0.3, recurrent_dropout=0.2)),\n",
    "        Bidirectional(RNNLayer(64,dropout=0.3, recurrent_dropout=0.2)),\n",
    "        Dropout(0.3),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_train_pad, y_train_eff_cat, epochs=10, batch_size=128, validation_data =(X_test_pad,y_test_eff_cat) ,verbose=1)\n",
    "\n",
    "    y_pred = np.argmax(model.predict(X_test_pad), axis=1)\n",
    "\n",
    "    evaluate_model(model, history, X_test_pad, y_test_labels, is_binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jkjA_XAdxiv3",
    "outputId": "56e52fb0-d73f-47fc-f9b8-8f18cd3c40cb"
   },
   "outputs": [],
   "source": [
    "train_multiclass_model(\"SimpleRNN Effectiveness\", SimpleRNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "CVlbpRpJxkZx",
    "outputId": "7309b782-5823-461e-9e11-46de83bb418e"
   },
   "outputs": [],
   "source": [
    "train_multiclass_model(\"LSTM Effectiveness\", LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3kWaxN-nxlG4",
    "outputId": "55065566-6898-4402-a00b-6bcc7db0f11a"
   },
   "outputs": [],
   "source": [
    "train_multiclass_model(\"GRU Effectiveness\", GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJ1wH_cOKQGr"
   },
   "source": [
    "In conclusion, when comparing RNN, LSTM, and GRU architectures, LSTM tends to perform better due to its more complex structure, including additional gates that allow it to better control the flow of information. This makes it more effective at capturing long-term dependencies in text. However, despite this advantage, the overall model performance remains modest â€” with LSTM achieving around 65% accuracy, it still misclassifies approximately 35% of the cases. Additionally, the model shows signs of overfitting, learning the training data well but struggling to generalize to new, unseen data.\n",
    "\n",
    "This performance limitation may be attributed to several factors: limited or imbalanced training data, noisy or unstructured text (e.g., spelling errors, informal language), insufficient preprocessing, overly simple model architecture, or the use of randomly initialized embeddings rather than pretrained ones.\n",
    "\n",
    "To improve results, several strategies could be applied: incorporating pretrained word embeddings (such as GloVe or Word2Vec),, expanding and cleaning the dataset, applying regularization techniques (like dropout or L2), fine-tuning hyperparameters, and ensuring stratified sampling to preserve class balance during training. These enhancements could help the model generalize better and increase classification performance"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
